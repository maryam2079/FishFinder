{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tu-nP6uHWKnt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AdpBUtKgRV8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "from scipy.fft import fft, fftfreq\n",
        "import statistics\n",
        "import scipy.integrate as integrate\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from scipy.integrate import trapz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import zipfile\n",
        "import shutil\n",
        "from scipy.stats import ttest_ind\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, f1_score, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "import seaborn as sns\n",
        "import heapq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read\n",
        "Read the game dataset that was gathered from the mobile game and fix the missing places"
      ],
      "metadata": {
        "id": "ajZeddtRgeZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In the recorded game datset there are empty spaces in the Type column, this function fills those empty cells\n",
        "def fill_type(data):\n",
        "  Value = data[0]['Type'].unique()\n",
        "  for df in data:\n",
        "    df['Type'] = df['Type'].replace(Value[1], method = 'ffill')\n",
        "    df['Type'] = df['Type'].replace(Value[4], Value[5])"
      ],
      "metadata": {
        "id": "viQMMICbEzUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extrac the 200 rows of the game dataset which belongs to the main phase which is the target for the analysis\n",
        "def main_data(data):\n",
        "  main = []\n",
        "  Value = data[0]['Type'].unique()\n",
        "  for df in data:\n",
        "    main.append(df[df['Type'] == Value[-3]])\n",
        "  return main"
      ],
      "metadata": {
        "id": "t6bQ_0X1E3Z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKd55ILCE80M"
      },
      "outputs": [],
      "source": [
        "#Create the main dataset\n",
        "def extract_main_test(filepath):\n",
        "  GAME = pd.read_excel(filepath, sheet_name = None)\n",
        "  game_data = list(GAME.values())\n",
        "  fill_type(game_data)\n",
        "  main_game = main_data(game_data)\n",
        "  return main_game"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IVA Features\n",
        "Definition of the IVA-2 features. These functions are defined based on the standard definition of IVA-2 features."
      ],
      "metadata": {
        "id": "tu-nP6uHWKnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def idx():\n",
        "  trial = 'AVAAVVVVAAVAAVAVVAVAAAVAVAVVVAVAAAVVAAVVAVAAAVAVVV'*8\n",
        "  auditory_idx = np.array([i.start() for i in re.finditer('A', trial)])\n",
        "  visual_idx = np.array([i.start() for i in re.finditer('V', trial)])\n",
        "  return [auditory_idx, visual_idx]"
      ],
      "metadata": {
        "id": "QoQsUKax9upe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Prudence(data):\n",
        "  auditory_prudence = []\n",
        "  visual_prudence = []\n",
        "  error_idx = np.array([6, 12, 19, 26, 31, 38, 44, 49, 51, 106, 112, 119, 126, 131, 138, 144, 149, 151,\n",
        "      206, 212, 219, 226, 231, 238, 244, 249, 251, 306, 312, 319, 326, 331, 338, 344, 349, 351,\n",
        "      52, 53, 152, 153, 252, 253, 352, 353, 59, 61, 68, 73, 80, 85, 87, 91, 155, 159, 161,\n",
        "      168, 173, 180, 185, 187, 191, 255, 259, 261, 268, 273, 280, 285, 287, 291,\n",
        "      355, 359, 361, 368, 373, 380, 385, 387, 3])-1\n",
        "  [auditory_idx, visual_idx] = idx()\n",
        "  aud_error = np.intersect1d(auditory_idx, error_idx)\n",
        "  vis_error = np.intersect1d(visual_idx, error_idx)\n",
        "  for df in data:\n",
        "    aud_data = df.iloc[list(aud_error)]\n",
        "    aud = aud_data[aud_data['Response'] == '  comission error  '].shape[0]\n",
        "    auditory_prudence.append(100-((aud/44)*100))\n",
        "\n",
        "    vis_data = df.iloc[list(vis_error)]\n",
        "    vis = vis_data[vis_data['Response'] == '  comission error  '].shape[0]\n",
        "    visual_prudence.append(100-((vis/35)*100))\n",
        "  return [auditory_prudence, visual_prudence]"
      ],
      "metadata": {
        "id": "sq1YUfKhFf-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Vigilance(data):\n",
        "  auditory_vigilance = []\n",
        "  visual_vigilance = []\n",
        "  error_idx = np.array([56, 62, 69, 76, 81, 88, 94, 99, 101, 102,\n",
        "              156, 162, 169, 176, 181, 188, 194, 199, 201, 202,\n",
        "              256, 262, 269, 276, 281, 288, 294, 299, 301, 302,\n",
        "              356, 362, 369, 376, 381, 388, 394, 399, 13, 20, 27, 32, 39, 45, 50,\n",
        "              107, 113, 120, 127, 132, 139, 145, 150,\n",
        "              207, 213, 220, 227, 232, 239, 245, 250,\n",
        "              307, 313, 320, 327, 332, 339, 345, 350])-1\n",
        "  [auditory_idx, visual_idx] = idx()\n",
        "  aud_error = np.intersect1d(auditory_idx, error_idx)\n",
        "  vis_error = np.intersect1d(visual_idx, error_idx)\n",
        "  for df in data:\n",
        "    aud_data = df.iloc[list(aud_error)]\n",
        "    aud = aud_data[aud_data['Response'] == '  omission error  '].shape[0]\n",
        "    auditory_vigilance.append(100-((aud/35)*100))\n",
        "\n",
        "    vis_data = df.iloc[list(vis_error)]\n",
        "    vis = vis_data[vis_data['Response'] == '  omission error  '].shape[0]\n",
        "    visual_vigilance.append(100-((vis/34)*100))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return [auditory_vigilance, visual_vigilance]"
      ],
      "metadata": {
        "id": "i3MpGZ6OKiSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Comprehension(data):\n",
        "  auditory_comp = []\n",
        "  visual_comp = []\n",
        "  error_idx = np.array([54, 55, 57, 58, 60, 63, 64, 65, 66, 67, 70, 71, 72, 74,\n",
        "                        75, 77, 78, 79, 82, 83, 84, 86, 89, 90, 92, 93, 95, 96, 97, 98, 100,\n",
        "                        154, 157, 158, 160, 163, 164, 165, 166, 167, 170, 171, 172, 174, 175,\n",
        "                        177, 178, 179, 182, 183, 184, 186, 189, 190, 192, 193, 195, 196, 197, 198, 200,\n",
        "                        254, 257, 258, 260, 263, 264, 265, 266, 267, 270, 271, 272, 274, 275, 277, 278, 279, 282,\n",
        "                        283, 284, 286, 289, 290, 292, 293, 295, 296, 297, 298, 300, 354, 357, 358, 360, 363, 364,\n",
        "                        365, 366, 367, 370, 371, 372, 374, 375, 377, 378, 379, 382, 383, 384, 386, 389, 390, 392, 393, 395,\n",
        "                        396, 397, 398, 400, 1, 2, 3, 4, 5, 8, 9, 10, 11, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25,\n",
        "                        28, 29, 30, 33, 34, 35, 36, 37, 40, 41, 42, 43, 46, 47, 48, 103, 104, 105, 108, 109, 110,\n",
        "                        111, 114, 115, 116, 117, 118, 121, 122, 123, 124, 125, 128, 129, 130, 133, 134, 135, 136,\n",
        "                        137, 140, 141, 142, 143, 146, 147, 148, 203, 204, 205, 208, 209, 210, 211, 214, 215, 216, 217,\n",
        "                        218, 221, 222, 223, 224, 225, 228, 229, 230, 233, 234, 235, 236, 237, 240, 241, 242, 243, 246, 247, 248,\n",
        "                        303, 304, 305, 308, 309, 310, 311, 314, 315, 316, 317, 318, 321, 322, 323, 324, 325, 328,\n",
        "                        329, 330, 333, 334, 335, 336, 337, 340, 341, 342, 343, 346, 347, 348])-1\n",
        "  [auditory_idx, visual_idx] = idx()\n",
        "  aud_error = np.intersect1d(auditory_idx, error_idx)\n",
        "  vis_error = np.intersect1d(visual_idx, error_idx)\n",
        "  for df in data:\n",
        "    aud_data = df.iloc[list(aud_error)]\n",
        "    aud = aud_data[(aud_data['Response'] == '  omission error  ') | (aud_data['Response'] == '  comission error  ')].shape[0]\n",
        "    auditory_comp.append(100-((aud/121)*100))\n",
        "\n",
        "    vis_data = df.iloc[list(vis_error)]\n",
        "    vis = vis_data[(vis_data['Response'] == '  omission error  ') | (vis_data['Response'] == '  comission error  ')].shape[0]\n",
        "    visual_comp.append(100-((vis/130)*100))\n",
        "\n",
        "\n",
        "\n",
        "  return [auditory_comp, visual_comp]"
      ],
      "metadata": {
        "id": "OdY4li4KNFlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Consistency(data):\n",
        "  consistencyA = []\n",
        "  consistencyV = []\n",
        "  for df in data:\n",
        "\n",
        "    reaction_timeA = list(df[((df['Response'] == '  Hit  ') | (df['Response'] == '  comission error  ')) &\n",
        "                 ((df['Mode'] == '  Fish Auditory  ') | (df['Mode'] == '  Shark Auditory  '))]['ReactionTime'])\n",
        "    q1_A = np.percentile(reaction_timeA, 25)\n",
        "    q3_A = np.percentile(reaction_timeA, 75)\n",
        "    consistencyA.append((q1_A/q3_A)*100)\n",
        "\n",
        "    reaction_timeV = list(df[((df['Response'] == '  Hit  ') | (df['Response'] == '  comission error  ')) &\n",
        "                 ((df['Mode'] == '  Fish Visual  ') | (df['Mode'] == '  Shark Visual  '))]['ReactionTime'])\n",
        "    q1_V = np.percentile(reaction_timeV, 25)\n",
        "    q3_V = np.percentile(reaction_timeV, 75)\n",
        "    consistencyV.append((q1_V/q3_V)*100)\n",
        "\n",
        "  return [consistencyA, consistencyV]"
      ],
      "metadata": {
        "id": "W7M9cPzKWBAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Stamina(data):\n",
        "  StaminaA = []\n",
        "  StaminaV = []\n",
        "  for df in data:\n",
        "    df1 = df.head(n = 200)\n",
        "    Auditory_mean1 = statistics.mean(list(df1[(df1['Response'] == '  Hit  ') &\n",
        "             ((df1['Mode'] == '  Fish Auditory  ') | (df1['Mode'] == '  Shark Auditory  '))]['ReactionTime']))\n",
        "\n",
        "    Visual_mean1 = statistics.mean(list(df1[(df1['Response'] == '  Hit  ') &\n",
        "                 ((df1['Mode'] == '  Fish Visual  ') | (df1['Mode'] == '  Shark Visual  '))]['ReactionTime']))\n",
        "\n",
        "    df2 = df.tail(n = 200)\n",
        "    Auditory_mean2 = statistics.mean(list(df2[(df2['Response'] == '  Hit  ') &\n",
        "                 ((df2['Mode'] == '  Fish Auditory  ') | (df2['Mode'] == '  Shark Auditory  '))]['ReactionTime']))\n",
        "\n",
        "\n",
        "    Visual_mean2 = statistics.mean(list(df2[(df2['Response'] == '  Hit  ') &\n",
        "                 ((df2['Mode'] == '  Fish Visual  ') | (df2['Mode'] == '  Shark Visual  '))]['ReactionTime']))\n",
        "\n",
        "    StaminaA.append((Auditory_mean1/Auditory_mean2)*100)\n",
        "    StaminaV.append((Visual_mean1/Visual_mean2)*100)\n",
        "\n",
        "  return [StaminaA, StaminaV]"
      ],
      "metadata": {
        "id": "6h2txiwa0gh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Focus(data):\n",
        "\n",
        "  FocusA = []\n",
        "  FocusV = []\n",
        "\n",
        "  for df in data:\n",
        "    reaction_timeA = list(df[((df['Response'] == '  Hit  ') | (df['Response'] == '  comission error  ')) &\n",
        "                 ((df['Mode'] == '  Fish Auditory  ') | (df['Mode'] == '  Shark Auditory  '))]['ReactionTime'])\n",
        "    FocusA.append((1 - (statistics.pstdev(reaction_timeA)/statistics.mean(reaction_timeA)))*100)\n",
        "\n",
        "    reaction_timeV = list(df[((df['Response'] == '  Hit  ') | (df['Response'] == '  comission error  ')) &\n",
        "                 ((df['Mode'] == '  Fish Visual  ') | (df['Mode'] == '  Shark Visual  '))]['ReactionTime'])\n",
        "    FocusV.append((1 - (statistics.pstdev(reaction_timeV)/statistics.mean(reaction_timeV)))*100)\n",
        "\n",
        "  return [FocusA, FocusV]"
      ],
      "metadata": {
        "id": "yeZC4oJi0oas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Speed(data):\n",
        "  SpeedA = []\n",
        "  SpeedV = []\n",
        "  for df in data:\n",
        "      reaction_timeA = list(df[((df['Response'] == '  Hit  ') &\n",
        "                 ((df['Mode'] == '  Fish Auditory  ') | (df['Mode'] == '  Shark Auditory  ')) &\n",
        "                  (df['middleMiddle'] == 1) & (df['ReactionTime'] > 0.125))]['ReactionTime'])\n",
        "      SpeedA.append(statistics.mean(reaction_timeA))\n",
        "\n",
        "      reaction_timeV = list(df[((df['Response'] == '  Hit  ') &\n",
        "                 ((df['Mode'] == '  Fish Visual  ') | (df['Mode'] == '  Shark Visual  ')) &\n",
        "                  (df['middleMiddle'] == 1) & (df['ReactionTime'] > 0.125))]['ReactionTime'])\n",
        "\n",
        "      SpeedV.append(statistics.mean(reaction_timeV))\n",
        "\n",
        "  return [SpeedA, SpeedV]"
      ],
      "metadata": {
        "id": "rwyz-l5-V2iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Dataset\n",
        "Functions that are used for creating the feature set"
      ],
      "metadata": {
        "id": "gS2DjK51gsnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the selected features\n",
        "def extract_features(data):\n",
        "  features = Consistency(data) + Stamina(data) + [Vigilance(data)[0]] + [Comprehension(data)[1]]\n",
        "  columns = ['Auditory Consistancy', 'Visual Consistancy', 'Auditory Stamina', 'Visual stamina', 'Auditory Vigilance', 'Visual Comprehension']\n",
        "  data_dict = dict(zip(columns, features))\n",
        "  dataset = pd.DataFrame.from_dict(data_dict)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "uNt1Z81KBKRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMr7r7A_XYuJ"
      },
      "source": [
        "#IVA Set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If the datset is for one person use this normalization function\n",
        "def normal_one(data):\n",
        "  min_values = [22.87627949072171, 20.65247385818543, 84.20707555042883, 85.38833884113552, 57.14285714285715, 51.53846153846154]\n",
        "  max_values = [86.88650721733065, 85.41685682241307, 150.9936660303136, 139.1899024268303, 100.0, 100.0]\n",
        "  norm_data = (data - min_values)/list(map(lambda a, b: a - b, max_values, min_values))\n",
        "  return norm_data"
      ],
      "metadata": {
        "id": "4CMWCleknU6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If the dataset is for multiple people use this normalization function\n",
        "def normalize(df):\n",
        "  scaler = MinMaxScaler()\n",
        "  cols = list(df.columns)\n",
        "  data = df[cols]\n",
        "  d = scaler.fit_transform(data)\n",
        "  normal_df = pd.DataFrame(d, columns=cols)\n",
        "  return normal_df"
      ],
      "metadata": {
        "id": "U0f7q3wCuIU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replace filepath with the path which the game dataset is stored\n",
        "filepath = '...'\n",
        "\n",
        "df = extract_features(extract_main_test(filepath))\n",
        "#normalize the data using either of those two normalization functions\n",
        "df_norm = normalization(df)"
      ],
      "metadata": {
        "id": "n_D1In8nuMxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Label Result"
      ],
      "metadata": {
        "id": "JxjpbXwap-Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "#replace the model_filepath with the path the model is stored\n",
        "game_model1 = pickle.load(open(\"model_filepath\", 'rb'))\n",
        "y_pred = game_model1.predict(df_norm)"
      ],
      "metadata": {
        "id": "gztoOrD1p1jR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}